{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r5yS7jxH9dy2"
      },
      "outputs": [],
      "source": [
        "#!pip install textract\n",
        "#!sudo apt-get install antiword"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Чтение документа"
      ],
      "metadata": {
        "id": "YSOToslT9yMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import textract\n",
        "import unicodedata\n",
        "import chardet\n",
        "import string\n",
        "\n",
        "def extract_text(file_path):\n",
        "    \"\"\"\n",
        "    Считывает текст по ссылке.\n",
        "\n",
        "    Параметры:\n",
        "    - file_path (str): ссылка на файл.\n",
        "\n",
        "    Возвращает:\n",
        "    - text (str): текст из документа.\n",
        "    \"\"\"\n",
        "    file_extension = file_path.split('.')[-1].lower()\n",
        "    if file_extension in ['docx','doc']:\n",
        "        text = unicodedata.normalize(\"NFKD\", textract.process(file_path).decode('utf-8'))\n",
        "    elif file_extension == 'pdf':\n",
        "        text = unicodedata.normalize(\"NFKD\", textract.process(file_path).decode('utf-8'))\n",
        "    else:\n",
        "        text = \"Формат файла не соответствует!\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "Z3wUiAyK90qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Структура ГОСТ"
      ],
      "metadata": {
        "id": "eFw_YrpO9_sa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def gost(text, number):\n",
        "  \"\"\"\n",
        "  Проверяет соответствие структуре ГОСТ.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст из документа,\n",
        "  - number (int): тип документа.\n",
        "\n",
        "  Возвращает:\n",
        "  - result_gost (list): список списков, содержащиц в себе элемент структуры ГОСТ и его наличие/отсутствие в документе.\n",
        "  \"\"\"\n",
        "  Tz_note = [\"общие сведения\", \"цели и назначение создания автоматизированной системы\", \"характеристика объектов автоматизации\",\n",
        "             \"требования к автоматизированной системе\", \"состав и содержание работ по созданию автоматизированной системы\",\n",
        "             \"порядок разработки автоматизированной системы\", \"порядок контроля и приемки автоматизированной системы\",\n",
        "             \"требования к составу и содержанию работ по подготовке объекта автоматизации к вводу автоматизированной системы в действие\",\n",
        "             \"требования к документированию\", \"источники разработки\"]\n",
        "  Tz_note_check = [\"общиесведения\",\"целииназначениесозданияавтоматизированноисистемы|назначениеицели(?:развития)?(?:и)?(?:создания)?(?:подсистем)?системы\",\n",
        "                   \"характеристикаобъектовавтоматизации|характеристикаобъектаавтоматизации\", \"требованияк(?:автоматизированнои)?системе\",\n",
        "                   \"состависодержаниеработ(?:услуг)?(?:посозданиюавтоматизированнойсистемы|поразвитиюсистемы)|требованияксоставу(?:исодержанию)?работ(?:посозданиюсистемы|результатамсрокамиэтапамихвыполнения)\",\n",
        "                   \"порядокразработкиавтоматизированноисистемы\", \"порядокконтроляиприемки(?:автоматизированнои)?(?:системы|работ)\",\n",
        "                   \"требованияксоставуисодержаниюработ(?:услуг)?поподготовкеобъектаавтоматизацииквводу(?:автоматизированнои)?системывдеиствие\", \"требованиякдокументированию\", \"источникиразработки\"]\n",
        "  Explanatory_note = [\"общие положения\", \"описание процесса деятельности\", \"основные технические решения\", \"мероприятия по подготовке объекта автоматизации к вводу системы в действие\"]\n",
        "  Explanatory_note_check = [\"общиеположения\", \"описаниепроцессадеятельности\", \"основныетехническиерешения\", \"мероприятияпоподготовкеобъектаавтоматизации(?:\\n)?квводусистемывдеиствие\"]\n",
        "  Report_note = [\"список исполнителей\", \"реферат\", \"содержание\", \"термины и определения\", \"перечень сокращений и обозначений\",\n",
        "                 \"введение\", \"заключение\", \"список использованных источников\", \"приложения\"]\n",
        "  Report_note_check = [\"списокисполнителеи\", \"реферат\", \"содержание\", \"терминыиопределения\", \"переченьсокращениииобозначении\",\n",
        "                       \"введение\", \"заключение\", \"списокиспользованныхисточников\", \"приложени(?:я|e)\"]\n",
        "  Report_note1 = [\"аннотация\", \"объект опытной эксплуатации\", \"цель опытной эксплуатации\",\n",
        "                  \"общие положения\", \"опытная эксплуатация\", \"порядок проведения опытной эксплуатации\", \"результаты проведения опытной эксплуатации\",\n",
        "                  \"cписок сокращений и обозначений\", \"заключение\"]\n",
        "  Report_note_check1 = [\"аннотация\", \"объектопытноиэксплуатации\", \"цельопытноиэксплуатации\", \"общиеположения\", \"опытнаяэксплуатация\",\n",
        "                        \"порядокпроведенияопытноиэксплуатации|журналопытноиэксплуатации\", \"результатыпроведенияопытноиэксплуатации\",\n",
        "                        \"списоксокращениииобозначении|определенияобозначенияисокращения\", \"заключение\"]\n",
        "  Check = [[Tz_note, Tz_note_check],[Explanatory_note, Explanatory_note_check],[Report_note, Report_note_check],[Report_note1, Report_note_check1]]\n",
        "  cleaned_text = re.sub(r'[^а-яА-Я\\n]', '', text.lower())\n",
        "  cleaned_text = re.sub(r'\\n+', '\\n', cleaned_text)\n",
        "  result_gost = []\n",
        "  for i, j in zip(Check[number][1], Check[number][0]):\n",
        "    if(len(re.findall('\\n{1}'+str(i)+'{1}\\n', cleaned_text)) >= 1):\n",
        "        result_gost.append([j, True])\n",
        "    else:\n",
        "        result_gost.append([j, True])\n",
        "  return result_gost"
      ],
      "metadata": {
        "id": "1z09d55j-YI_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Реализация антиплагиата"
      ],
      "metadata": {
        "id": "tXM7v_uqGDgg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Очищение текста"
      ],
      "metadata": {
        "id": "svl6iIbfA_6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Очищает текст от элементов, которые не являются знакаму пунктуации, буквами или цифрами.\n",
        "\n",
        "    Параметры:\n",
        "    - text (str): текст из документа.\n",
        "\n",
        "    Возвращает:\n",
        "    - text (str): очищеннный текст документа.\n",
        "    \"\"\"\n",
        "    text = re.sub(r'(\\W)\\1+', r'\\1', text)\n",
        "    text = re.sub(r'[^a-zA-Zа-яА-ЯёЁ0-9{}_„“«»/\\-‘’]+'.format(re.escape(string.punctuation)), ' ', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "Mwau4qdEA_eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запрос на проверку плагиата"
      ],
      "metadata": {
        "id": "oXnSpGhZBZbs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "def anti_plagiarism(userkey, text):\n",
        "  \"\"\"\n",
        "  Возвразает uid запроса.\n",
        "\n",
        "  Параметры:\n",
        "  - userkey (str): ключ доступа для пользователя,\n",
        "  - text (str): текст из документа.\n",
        "\n",
        "  Возвращает в случае успеха:\n",
        "  - response_data (dict): содержит uid запроса.\n",
        "  Возвращает в случае неудачи:\n",
        "  - (int): код ошибки.\n",
        "  \"\"\"\n",
        "  # Данные для запроса\n",
        "  body = {\n",
        "      'userkey': userkey,\n",
        "      'text': text,\n",
        "      'callback': 'https://your-site.com/callback'}\n",
        "  response = requests.post('https://api.text.ru/post', json=body)\n",
        "  # Проверка успешности запроса\n",
        "  if response.status_code != 200:\n",
        "    raise Exception(f'Ошибка запроса: {response.status_code}')\n",
        "    return response.status_code\n",
        "  response_data = response.json()\n",
        "  return response_data"
      ],
      "metadata": {
        "id": "5D_neJpLBaLH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def anti_plagiarism_result(userkey, uid):\n",
        "  \"\"\"\n",
        "  Возвразает информацию из антиплагиата.\n",
        "\n",
        "  Параметры:\n",
        "  - userkey (str): ключ доступа для пользователя,\n",
        "  - uid (str): uid запроса.\n",
        "\n",
        "  Возвращает:\n",
        "  - data (dict): содержит информацию из антиплагиата.\n",
        "  \"\"\"\n",
        "  body = {\n",
        "      'userkey': userkey,\n",
        "      'uid': uid}\n",
        "  url = 'https://api.text.ru/post'\n",
        "  response = requests.post(url, json=body)\n",
        "  response.raise_for_status()\n",
        "  data = response.json()\n",
        "  return data"
      ],
      "metadata": {
        "id": "TQSBGu5RBxKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_anti_plagiarism(userkey, text):\n",
        "  \"\"\"\n",
        "  Возвразает обработанную информацию из антиплагиата.\n",
        "\n",
        "  Параметры:\n",
        "  - userkey (str): ключ доступа для пользователя,\n",
        "  - text (str): текст документа.\n",
        "\n",
        "  Возвращает в случае успеха:\n",
        "  - (list): содержащий в себе информацию из антиплагиата, датафрейм найденных заимствований и значение уникальности текста.\n",
        "  Возвращает в случае неудачи:\n",
        "  - (str): информацию о произошедшей ошибке.\n",
        "  \"\"\"\n",
        "  post = anti_plagiarism(userkey, text)\n",
        "  df = pd.DataFrame()\n",
        "  if \"error_code\" in post and \"error_desc\" in post:\n",
        "    return f\"Произошла ошибка {post['error_code']}: {post['error_desc']}\", df\n",
        "  else:\n",
        "    answer = anti_plagiarism_result(userkey, post['text_uid'])\n",
        "    while \"error_code\" in answer:\n",
        "      answer = anti_plagiarism_result(userkey, post['text_uid'])\n",
        "    text_unique = answer['text_unique']\n",
        "    result_json = json.loads(answer['result_json'])\n",
        "    res = []\n",
        "    res.append(f\"Дата окончания проверки текста на сервере: {result_json['date_check']}\\n\")\n",
        "    res.append(f\"Уникальность текста: {text_unique}%\\n\")\n",
        "    res_str = \"\"\n",
        "    if len(result_json['urls']) != 0:\n",
        "      urls_pd = []\n",
        "      pro_pd = []\n",
        "      res.append(f\"Список URL и процент совпадения текста:\\n\")\n",
        "      for url_data in result_json['urls']:\n",
        "        ur = url_data['url']\n",
        "        res_str += f\"URL: {ur}\\n\"\n",
        "        urls_pd.append(ur)\n",
        "        pr = url_data['plagiat']\n",
        "        res_str += f\"Процент совпадения текста: {pr}%\\n\"\n",
        "        pro_pd.append(pr)\n",
        "      if len(urls_pd) != 0:\n",
        "        list_tuples = list(zip(urls_pd, pro_pd))\n",
        "        df = pd.DataFrame(list_tuples, columns=['URL', '%'])\n",
        "    res.append(res_str)\n",
        "    return res, df, text_unique"
      ],
      "metadata": {
        "id": "evUV89UYCAYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Выделение ключевых и частовстречающихся слов из текста"
      ],
      "metadata": {
        "id": "yrksqEi_EwPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Часто встречающиеся слова"
      ],
      "metadata": {
        "id": "5Mdvzdh9FyhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODca2N2KE3uP",
        "outputId": "df2c3039-5812-4dee-8b1c-86917dc040f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "from pymystem3 import Mystem\n",
        "from nltk.probability import FreqDist\n",
        "\n",
        "def remove_chars_from_text(text, chars):\n",
        "  \"\"\"\n",
        "  Удаляет элементы из текста.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст документа,\n",
        "  - chars (str): строка содержащая данные, которые нужно удалить.\n",
        "\n",
        "  Возвращает:\n",
        "  - (str): очищенную строку.\n",
        "  \"\"\"\n",
        "  return \"\".join([ch for ch in text if ch not in chars])\n",
        "\n",
        "def Text_processing(text):\n",
        "  \"\"\"\n",
        "  Подсчитывает часто встречающиеся слова из текста.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст документа.\n",
        "\n",
        "  Возвращает:\n",
        "  - (class): счетчик распределения частот NLTK.\n",
        "  \"\"\"\n",
        "  text = text.lower()\n",
        "  mystem = Mystem()\n",
        "  stops_word = [\"менее\", \"более\", \"кроме\", \"должно\", \"должный\", \"данный\", \"детея\"] + stopwords.words(\"russian\") + stopwords.words(\"english\")\n",
        "  text = remove_chars_from_text(text, string.punctuation)  #удаление знаков пунктуации\n",
        "  text = remove_chars_from_text(text, string.digits)  #удаление цифр\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  text = tokenizer.tokenize(text)\n",
        "  text = [mystem.lemmatize(word)[0] for word in text] #лемматизируем текст\n",
        "  text = [i for i in text if i not in stops_word] #удаление стоп слов\n",
        "  text = nltk.Text(text)\n",
        "  return FreqDist(text)"
      ],
      "metadata": {
        "id": "4cigDPWEE8hE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def word_cloud(data):\n",
        "  \"\"\"\n",
        "  Рисует облако слов.\n",
        "\n",
        "  Параметры:\n",
        "  - data (object): датафрейм содержащий слово и число, которое равно количеству появления слова в документе.\n",
        "  \"\"\"\n",
        "  # Создание текста для облака слов\n",
        "  text = ' '.join(data)\n",
        "  # Создание облака слов\n",
        "  wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "  # Отображение облака слов\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.imshow(wordcloud, interpolation='bilinear')\n",
        "  plt.axis('off')\n",
        "  # Сохранение облака слов в формате PNG\n",
        "  plt.savefig('wordcloud.png', bbox_inches='tight', pad_inches=0)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "yVy2pJs9FQRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод RAKE"
      ],
      "metadata": {
        "id": "l-8fzK7vF1jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nlp-rake\n",
        "#!pip install nltk"
      ],
      "metadata": {
        "id": "GwnC9wOfF2x4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nlp_rake import Rake\n",
        "\n",
        "def key_word_rake(text, stops_my, n, max_words):\n",
        "  \"\"\"\n",
        "  Возвращает ключевые слова, используя метод RAKE.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст документа,\n",
        "  - stops_my (list): стоп-слова,\n",
        "  - n (int): количество возвращаемых слов,\n",
        "  - max_words (int): максимальное числов слов в фразе.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): n найденных слов.\n",
        "  \"\"\"\n",
        "  text = clean_text(text)\n",
        "  text = re.sub(r'\\s+\\.', '.', text)\n",
        "  text = remove_chars_from_text(text, string.digits)  #удаление цифр\n",
        "  text = re.sub(r'\\b\\w{2}\\b', '', text)\n",
        "  rake = Rake(stopwords = stops_my, max_words = max_words)\n",
        "  k = rake.apply(text)\n",
        "  return k[:n]"
      ],
      "metadata": {
        "id": "0OPDmpI8GNwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод YAKE"
      ],
      "metadata": {
        "id": "LDHakdooHaaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install yake"
      ],
      "metadata": {
        "id": "ddQeaAIcIPk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yake\n",
        "\n",
        "def key_word_yake(text, n, top):\n",
        "  \"\"\"\n",
        "  Возвращает ключевые слова, используя метод YAKE.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст документа,\n",
        "  - n (int): максимальное количество слов в фразе,\n",
        "  - top (int): количество ключевых слов.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): n найденных слов.\n",
        "  \"\"\"\n",
        "  extractor = yake.KeywordExtractor(\n",
        "      lan = \"ru\",\n",
        "      n = n,\n",
        "      dedupLim = 0.7,\n",
        "      top = top\n",
        "      )\n",
        "  keywords = extractor.extract_keywords(text)\n",
        "  filtered_keywords = [kw for kw in keywords if len(kw[0]) > 2]\n",
        "  return sorted(filtered_keywords, key=lambda x: x[1], reverse=True)"
      ],
      "metadata": {
        "id": "TXQRhOdYHhRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextRank"
      ],
      "metadata": {
        "id": "cO7stPe_ILl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install summa"
      ],
      "metadata": {
        "id": "AzNvZRT8IENw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from summa import keywords\n",
        "from pymystem3 import Mystem\n",
        "\n",
        "def key_word_textrank(text, stops, n):\n",
        "  \"\"\"\n",
        "  Возвращает ключевые слова, используя метод TextRank.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст документа,\n",
        "  - stops (list): стоп слова,\n",
        "  - n (int): количество ключевых слов.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): n найденных слов.\n",
        "  \"\"\"\n",
        "  mystem = Mystem()\n",
        "  text_clean = \"\"\n",
        "  for word in text.split():\n",
        "    lemma = mystem.lemmatize(word)[0]\n",
        "    if lemma.lower() not in stops and len(lemma) > 2:\n",
        "        text_clean += lemma + \" \"\n",
        "  return keywords.keywords(text_clean, language = \"russian\").split(\"\\n\")[:n]"
      ],
      "metadata": {
        "id": "iYHDQhFmIZMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topia"
      ],
      "metadata": {
        "id": "yH3EBykFIw2C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install rutermextract"
      ],
      "metadata": {
        "id": "R0KVJmq1ItIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rutermextract import TermExtractor\n",
        "\n",
        "def topia(cleaned_text, n):\n",
        "  \"\"\"\n",
        "  Возвращает ключевые слова, используя метод Topia.\n",
        "\n",
        "  Параметры:\n",
        "  - cleaned_text (str): текст документа,\n",
        "  - n (int): количество ключевых слов.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): содержащий в себе три элемента массив n ключевых слов, массив ключевых слов с их количеством и массив ключевых фраз с их количеством.\n",
        "  \"\"\"\n",
        "  term_extractor = TermExtractor()\n",
        "  single_word_terms = []\n",
        "  key_word4 = []\n",
        "  multi_word_terms = []\n",
        "  for term in term_extractor(cleaned_text):\n",
        "    if (re.search(r'\\d', term.normalized) is not None) == False:\n",
        "      if len(term.normalized.split()) == 1:\n",
        "        single_word_terms.append((term.normalized, term.count))\n",
        "        key_word4.append(term.normalized)\n",
        "      else:\n",
        "        multi_word_terms.append((term.normalized, term.count))\n",
        "  return key_word4[:n], single_word_terms, multi_word_terms"
      ],
      "metadata": {
        "id": "SExONIJaIx__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Исправление слов используя YandexSpeller"
      ],
      "metadata": {
        "id": "ep0yqFF-KJe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pyaspeller"
      ],
      "metadata": {
        "id": "hsQWbXHVKOlY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyaspeller import YandexSpeller\n",
        "\n",
        "def lemmatize_words(words):\n",
        "  \"\"\"\n",
        "  Исправляет ошибки в написании слов.\n",
        "\n",
        "  Параметры:\n",
        "  - words (list): найденные слова.\n",
        "\n",
        "  Возвращает:\n",
        "  - (set): уникальные слова.\n",
        "  \"\"\"\n",
        "  mystem = Mystem()\n",
        "  speller = YandexSpeller()\n",
        "  cleaned_words = []\n",
        "  for i in words:\n",
        "    new_i = re.sub(r'[^a-zA-Z0-9а-яА-Я]', '', i)\n",
        "    if new_i != \"\":\n",
        "      cleaned_words.append(new_i)\n",
        "  lemmatized = mystem.lemmatize(\" \".join(cleaned_words))\n",
        "  return set(speller.spelled(word.strip()) for word in lemmatized if word.strip())"
      ],
      "metadata": {
        "id": "EnqzFOKbKTT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###РОСПАТЕНТ"
      ],
      "metadata": {
        "id": "jkEd7OmBK0-c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для распаковки имен из словарей\n",
        "def unpack_names(names):\n",
        "    \"\"\"\n",
        "    Преобразует данные в строку.\n",
        "\n",
        "    Параметры:\n",
        "    - names (list/dict/str): данные.\n",
        "\n",
        "    Возвращает:\n",
        "    - (set): преобразованные данные в виде строки.\n",
        "    \"\"\"\n",
        "    if isinstance(names, list):\n",
        "        return ', '.join(item['name'] for item in names)\n",
        "    elif isinstance(names, dict):\n",
        "        return names['name']\n",
        "    else:\n",
        "        return str(names)\n",
        "\n",
        "# Функция для удаления <em> и </em> из строк\n",
        "def remove_em_tags(value):\n",
        "    \"\"\"\n",
        "    Удаляет элементы из строки.\n",
        "\n",
        "    Параметры:\n",
        "    - value (str): данные.\n",
        "\n",
        "    Возвращает:\n",
        "    - (str): очищенная строка.\n",
        "    \"\"\"\n",
        "    if isinstance(value, str):\n",
        "        return value.replace('<em>', '').replace('</em>', '')\n",
        "    else:\n",
        "        return value\n",
        "\n",
        "def patent_search(q, limit):\n",
        "  \"\"\"\n",
        "  Выполняет поиск по базе данных Роспатент.\n",
        "\n",
        "  Параметры:\n",
        "  - q (str): слово или фраза, по которой ищем,\n",
        "  - limit (int): количество возвращаемых документов.\n",
        "\n",
        "  Возвращает в случае успеха:\n",
        "  - (list): содержит информацию об статусе запроса удачный/нет, количество найденных документов, доступных и датафрейм с информацией.\n",
        "  Возвращает в случае неудачи:\n",
        "  - (list): содержит информацию об статусе запроса удачный/нет, ошибку, пустой датафрейм.\n",
        "  \"\"\"\n",
        "  url = 'https://searchplatform.rospatent.gov.ru/patsearch/v0.2/search'\n",
        "  headers = {\n",
        "      'Authorization': 'ВАШИ ДАННЫЕ',\n",
        "      'Content-Type': 'application/json'\n",
        "      }\n",
        "  data = {\n",
        "    'q': q,\n",
        "    'limit' : limit\n",
        "  }\n",
        "  try:\n",
        "    flag = False\n",
        "    response = requests.post(url, headers=headers, json=data)\n",
        "    response.raise_for_status()\n",
        "    flag = True\n",
        "    ans = response.json()\n",
        "    ans_pd = pd.DataFrame()\n",
        "    if ans[\"total\"] != 0 and ans[\"available\"] != 0:\n",
        "      ans_pd = pd.json_normalize(ans[\"hits\"])\n",
        "      column = ['id', 'common.document_number', 'snippet.title', 'biblio.ru.inventor', 'snippet.applicant', 'snippet.patentee', 'biblio.ru.patentee']\n",
        "      ans_pd = ans_pd.reindex(columns=column)\n",
        "      # Применение функции к указанным столбцам\n",
        "      columns_to_apply = ['biblio.ru.inventor', 'biblio.ru.patentee']\n",
        "      ans_pd[columns_to_apply] = ans_pd[columns_to_apply].applymap(unpack_names)\n",
        "      ans_pd = ans_pd.applymap(remove_em_tags)\n",
        "    return flag, ans[\"total\"], ans[\"available\"], ans_pd\n",
        "  except requests.exceptions.HTTPError as err:\n",
        "    return flag, err, pd.DataFrame()\n",
        "  except requests.exceptions.RequestException as err:\n",
        "    return flag, err, pd.DataFrame()"
      ],
      "metadata": {
        "id": "Z6pKcXJIK3T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def info_patent(key_word, multi_key, often_word):\n",
        "  \"\"\"\n",
        "  Выполняет поиск по базе данных Роспатент.\n",
        "\n",
        "  Параметры:\n",
        "  - key_word (list): ключевые слова,\n",
        "  - multi_key (list): ключевые фразы,\n",
        "  - often_word (list): часто встречающиеся слова.\n",
        "\n",
        "  Возвращает:\n",
        "  - dict_patent (dict): содержит информацию о выполненных поисках в базе.\n",
        "  \"\"\"\n",
        "  dict_patent = {}\n",
        "  limit = 5\n",
        "  dict_patent[\"all_keywords\"] = patent_search(\" AND \".join(key_word), limit)\n",
        "  if len(key_word) >= 5:\n",
        "    dict_patent[\"five_keywords\"] = patent_search(\" AND \".join(often_word[:5]), limit)\n",
        "  for i in key_word:\n",
        "    dict_patent[i] = patent_search(i, 1)\n",
        "  dict_patent[\"or_multi\"] = patent_search(\" OR \".join(key_word), limit)\n",
        "  return dict_patent"
      ],
      "metadata": {
        "id": "ssVTjcDpLDMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###OpenAlex"
      ],
      "metadata": {
        "id": "lSy9boR4Mqlr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def openalex(key, num=1):\n",
        "  \"\"\"\n",
        "  Выполняет поиск по базе данных OpenAlex.\n",
        "\n",
        "  Параметры:\n",
        "  - key (list): ключевые слова,\n",
        "  - num (int): количество возвращаемых значений по ключевому слову.\n",
        "\n",
        "  Возвращает:\n",
        "  - result_df (DataFrame): содержит информацию о выполненных поисках в базе.\n",
        "  \"\"\"\n",
        "  columns = [\"Слово\", \"doi\",  \"Заглавие\", 'Индексируется в', 'Наличие полного текста', 'api url']\n",
        "  url = f\"https://api.openalex.org/works?sample={num}\"\n",
        "  result_df = pd.DataFrame(columns=columns)\n",
        "  # Параметры запроса\n",
        "  for i in key:\n",
        "    params = {\"filter\": f\"title.search:{i}\"}\n",
        "    # Выполнение запроса\n",
        "    response = requests.get(url, params=params)\n",
        "    # Проверка статуса ответа\n",
        "    if response.status_code == 200:\n",
        "      data = response.json()\n",
        "      df_open = pd.DataFrame.from_dict(data[\"results\"])\n",
        "      df_open = df_open.reindex(columns=[\"doi\",  \"title\", 'indexed_in', 'has_fulltext', 'cited_by_api_url'])\n",
        "      df_open = df_open[[\"doi\",  \"title\", 'indexed_in', 'has_fulltext', 'cited_by_api_url']]\n",
        "      df_open['indexed_in'] = df_open['indexed_in'].apply(lambda x: \", \".join(x))\n",
        "      if len(df_open) != 0:\n",
        "        new_row = pd.DataFrame([[i] + list(df_open.iloc[0])], columns=result_df.columns)\n",
        "        result_df = pd.concat([new_row, result_df], ignore_index=True)\n",
        "        result_df = result_df.replace(False, \"Нет\").fillna(\"\")\n",
        "    else:\n",
        "      print(f\"Ошибка при выполнении запроса: для слова {i}\", response.status_code)\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "OGTRGm6XMtFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Поиск именнованных сущностей"
      ],
      "metadata": {
        "id": "mxymVWMCLysh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install natasha"
      ],
      "metadata": {
        "id": "8GWEhBnVL2K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import natasha\n",
        "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger, PER, LOC, ORG, NamesExtractor, AddrExtractor, Doc\n",
        "\n",
        "def name_org_loc(Type_poisk, segmenter, morph_tagger, syntax_parser, ner_tagger, morph_vocab, extractor, text):\n",
        "  \"\"\"\n",
        "  Выполняет поиск именнованных сущностей.\n",
        "\n",
        "  Параметры:\n",
        "  - Type_poisk (str): тип данных, который ищем,\n",
        "  - segmenter (natasha.segment.Segmenter): класс для сегментирования текста на токены,\n",
        "  - morph_tagger (natasha.morph.tagger.NewsMorphTagger): морфологический теггер,\n",
        "  - syntax_parser (natasha.syntax.NewsSyntaxParser): синтаксический парсер,\n",
        "  - ner_tagger (natasha.morph.vocab.MorphVocab): словарный запас морфологических единиц,\n",
        "  - extractor (natasha.extractors.AddrExtractor): словарный запас адресов,\n",
        "  - text (str): текст.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): найденные данные.\n",
        "  \"\"\"\n",
        "  doc = Doc(text)\n",
        "  doc.segment(segmenter)\n",
        "  doc.tag_morph(morph_tagger)\n",
        "  doc.parse_syntax(syntax_parser)\n",
        "  doc.tag_ner(ner_tagger)\n",
        "  for span in doc.spans:\n",
        "    span.normalize(morph_vocab)\n",
        "  for token in doc.tokens:\n",
        "    token.lemmatize(morph_vocab)\n",
        "  for span in doc.spans:\n",
        "    if span.type == Type_poisk:\n",
        "      span.extract_fact(extractor)\n",
        "  res_dict = {_.normal: _.fact.as_dict for _ in doc.spans if _.fact}\n",
        "  return list(res_dict.keys())"
      ],
      "metadata": {
        "id": "xo_LFujsL-x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def entities(text):\n",
        "  \"\"\"\n",
        "  Выполняет поиск именнованных сущностей PER, LOC, ORG.\n",
        "\n",
        "  Параметры:\n",
        "  - text (str): текст.\n",
        "\n",
        "  Возвращает:\n",
        "  - (list): найденные данные.\n",
        "  \"\"\"\n",
        "  segmenter = Segmenter()\n",
        "  morph_vocab = MorphVocab()\n",
        "  emb = NewsEmbedding()\n",
        "  morph_tagger = NewsMorphTagger(emb)\n",
        "  syntax_parser = NewsSyntaxParser(emb)\n",
        "  ner_tagger = NewsNERTagger(emb)\n",
        "  names_extractor = NamesExtractor(morph_vocab)\n",
        "  addr_extractor = AddrExtractor(morph_vocab)\n",
        "  res = []\n",
        "  for item1, item2 in zip([PER, LOC, ORG], [names_extractor, addr_extractor, names_extractor]):\n",
        "    res.append(name_org_loc(item1, segmenter, morph_tagger, syntax_parser, ner_tagger, morph_vocab, item2, text))\n",
        "  return res"
      ],
      "metadata": {
        "id": "t1GdzjwuMCOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ФОРМИРОВАНИЕ ОТЧЕТА ИЗ ДАННЫХ"
      ],
      "metadata": {
        "id": "6ZWqI12XM9Fb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install python-docx\n",
        "#!pip install aspose-words"
      ],
      "metadata": {
        "id": "5evhlHY1NBeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_docx(answer, doc):\n",
        "    \"\"\"\n",
        "    Функция добавления таблицы в отчет.\n",
        "    \"\"\"\n",
        "    table = doc.add_table(1, len(answer.columns))\n",
        "    table.style = 'Light Grid Accent 1'\n",
        "    head_cells = table.rows[0].cells\n",
        "    for i, item in enumerate(answer.columns):\n",
        "        p = head_cells[i].paragraphs[0]\n",
        "        p.add_run(item).bold = True\n",
        "        p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "\n",
        "    for _, row in answer.iterrows():\n",
        "        cells = table.add_row().cells\n",
        "        for i, item in enumerate(row):\n",
        "            cells[i].text = str(item)"
      ],
      "metadata": {
        "id": "NUsU8JfHNMUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def table_patent(dict_patent, paragraph, doc):\n",
        "  \"\"\"\n",
        "  Функция добавления данных из Роспатента.\n",
        "  \"\"\"\n",
        "  if dict_patent[0] == False:\n",
        "    run = paragraph.add_run(f\"ОШИБКА: {dict_patent[0]}\")\n",
        "  else:\n",
        "    run = paragraph.add_run(f\"Количество найденных документов: {dict_patent[1]}\\n\")\n",
        "    run = paragraph.add_run(f\"Количество доступных документов: {dict_patent[2]}\")\n",
        "    if dict_patent[1] != 0 and dict_patent[2] != 0:\n",
        "      data = dict_patent[3]\n",
        "      data = data.replace(\"nan\", \"\").fillna(\"\")\n",
        "      data.columns = [\"id\", \"Номер документа\", \"Название\", \"Изобретатель\", \"Заявитель\", \"Патентообладатель\" ,\"Патентообладатели\"]\n",
        "      table = doc.add_table(1, len(data.columns))\n",
        "      table.style = 'Light Grid Accent 1'\n",
        "      head_cells = table.rows[0].cells\n",
        "      for i, item in enumerate(data.columns):\n",
        "        p = head_cells[i].paragraphs[0]\n",
        "        p.add_run(item).bold = True\n",
        "        p.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "      for _, row in data.iterrows():\n",
        "        cells = table.add_row().cells\n",
        "        for i, item in enumerate(row):\n",
        "          cells[i].text = str(item)"
      ],
      "metadata": {
        "id": "bT_831YUNOpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def par_docx(doc, dict_patent):\n",
        "  \"\"\"\n",
        "  Функция добавления данных из Роспатента.\n",
        "  \"\"\"\n",
        "  key_info = []\n",
        "  for key, value in dict_patent.items():\n",
        "    if key == \"all_keywords\":\n",
        "      paragraph = doc.add_paragraph()\n",
        "      run = paragraph.add_run(\"Информация по патентам в которых встречается каждое ключевое слово:\\n\")\n",
        "      run.font.color.rgb = RGBColor(0, 240, 0)\n",
        "      table_patent(value, paragraph, doc)\n",
        "    elif key == \"five_keywords\":\n",
        "      paragraph1 = doc.add_paragraph()\n",
        "      run1 = paragraph1.add_run(\"Информация по патентам в которых встречается 5 самых частовстречающися слов:\\n\")\n",
        "      run1.font.color.rgb = RGBColor(0, 240, 0)\n",
        "      table_patent(value, paragraph1, doc)\n",
        "    elif key == \"or_multi\":\n",
        "      paragraph2 = doc.add_paragraph()\n",
        "      run2 = paragraph2.add_run(\"Информация по патентам в которых встречается одна из ключевых фраз:\\n\")\n",
        "      run2.font.color.rgb = RGBColor(0, 240, 0)\n",
        "      table_patent(value, paragraph2, doc)\n",
        "    else:\n",
        "      if value[0] != False and (value[1] != 0 and value[2] != 0):\n",
        "        key_info.append([key, value[1], value[2], value[3]])\n",
        "      else:\n",
        "        print(key, value[0], value[1], value[2])\n",
        "  columns = ['Ключевое слово', 'Найденные документы', 'Доступные документы', \"id\", \"Номер документа\", \"Название\", \"Изобретатель\", \"Заявитель\", \"Патентообладатель\" ,\"Патентообладатели\"]\n",
        "  result_df = pd.DataFrame(columns=columns)\n",
        "  for array in key_info:\n",
        "    new_row = pd.DataFrame([[array[0], array[1], array[2]] + list(array[3].iloc[0])], columns=result_df.columns)\n",
        "    result_df = pd.concat([new_row, result_df], ignore_index=True)\n",
        "  paragraph3 = doc.add_paragraph()\n",
        "  run3 = paragraph3.add_run(\"\\nИнформация по патентам в которых встречается одно ключевое слово:\")\n",
        "  run3.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  result_df = result_df.replace(\"nan\", \"\").fillna(\"\")\n",
        "  table_docx(result_df, doc)"
      ],
      "metadata": {
        "id": "aBz8_Xf8NR49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from docx import Document\n",
        "from docx.shared import Pt, RGBColor\n",
        "from docx.enum.text import WD_ALIGN_PARAGRAPH\n",
        "import aspose.words as aw\n",
        "from docx.shared import Inches\n",
        "\n",
        "\n",
        "def report_docx(result_gost, answer, df_often, key_word, multi_df, patent, arr_entitie, data_openalex, patent_flag, data_openalex_flag):\n",
        "  \"\"\"\n",
        "  Функция формирования отчета.\n",
        "  \"\"\"\n",
        "  substrings = [\"Дата окончания проверки текста на сервере:\", \"Уникальность текста:\", \"Список URL и процент совпадения текста:\", \"Произошла ошибка\"]\n",
        "  doc = Document()\n",
        "  style = doc.styles['Normal']\n",
        "  style.font.name = 'Arial'\n",
        "  style.font.size = Pt(11)\n",
        "  head = doc.add_heading('Отчет автоматической проверки текста')\n",
        "  head.style.font.size = Pt(20)\n",
        "  head.alignment = WD_ALIGN_PARAGRAPH.CENTER\n",
        "  head.style.paragraph_format.space_before = Pt(0)\n",
        "  paragraph = doc.add_paragraph()\n",
        "  run = paragraph.add_run('Информация о соответствии структуре ГОСТ\\n')\n",
        "  run.font.size = Pt(16)\n",
        "  run.bold = True\n",
        "  run.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  for ans in result_gost:\n",
        "    run_1 = paragraph.add_run(f\"{ans[0]}: \")\n",
        "    if ans[1] == True:\n",
        "      run_2 = paragraph.add_run(f\"Да\")\n",
        "    else:\n",
        "      run_2 = paragraph.add_run(f\"Нет\")\n",
        "    if ans[0] != result_gost[-1][0]:\n",
        "      paragraph.add_run(\"\\n\")\n",
        "  paragraph = doc.add_paragraph()\n",
        "  run = paragraph.add_run('Информация из антиплагиата\\n')\n",
        "  run.font.size = Pt(16)\n",
        "  run.bold = True\n",
        "  run.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  if \"Произошла ошибка\" not in answer[0]:\n",
        "    for ans in answer[0]:\n",
        "      for st in substrings:\n",
        "        if st in ans:\n",
        "          cur_str = ans.replace(st, \"\")\n",
        "          run_1 = paragraph.add_run(st)\n",
        "          run_1.font.bold = True\n",
        "          if cur_str != \"\\n\":\n",
        "            paragraph.add_run(cur_str)\n",
        "          break\n",
        "    table_docx(answer[1], doc)\n",
        "  else:\n",
        "    run_1 = paragraph.add_run(f\"{answer[0]}\\n\")\n",
        "  paragraph = doc.add_paragraph()\n",
        "  run = paragraph.add_run('Ключевые слова и фразы из текста\\n')\n",
        "  run.font.size = Pt(16)\n",
        "  run.bold = True\n",
        "  run.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  run = paragraph.add_run('Облако слов\\n')\n",
        "  run.add_picture('wordcloud.png', width=Inches(5))\n",
        "  run = paragraph.add_run(\"\\n10 самых частовстречающихся слов: \")\n",
        "  run.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run.italic = True\n",
        "  paragraph.add_run(', '.join(list(df_often[\"word\"])[:10]))\n",
        "  run1 = paragraph.add_run(\"\\nКлючевые слова: \")\n",
        "  run1.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run1.italic = True\n",
        "  paragraph.add_run(', '.join(key_word))\n",
        "  run2 = paragraph.add_run(\"\\nКлючевые словосочетания: \")\n",
        "  run2.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run2.italic = True\n",
        "  paragraph.add_run(', '.join(multi_df[\"word\"]))\n",
        "  run3 = paragraph.add_run(\"\\nДанные найденные по ключевым словам из Роспатента\")\n",
        "  run3.font.size = Pt(16)\n",
        "  run3.bold = True\n",
        "  run3.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  if \"Нет ключевых слов и фраз для поиска.\" == patent_flag:\n",
        "    paragraph.add_run(\"\\nНет ключевых слов и фраз для поиска.\\n\")\n",
        "  else:\n",
        "    k = par_docx(doc, patent)\n",
        "  paragraph = doc.add_paragraph()\n",
        "  run4 = paragraph.add_run('\\nЛокации, ФИО и организации из текста\\n')\n",
        "  run4.font.size = Pt(16)\n",
        "  run4.bold = True\n",
        "  run4.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  run5 = paragraph.add_run('\\nНайденные ФИО: ')\n",
        "  run5.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run5.italic = True\n",
        "  paragraph.add_run(', '.join(arr_entitie[0]))\n",
        "  run6 = paragraph.add_run('\\nНайденные локации: ')\n",
        "  run6.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run6.italic = True\n",
        "  paragraph.add_run(', '.join(arr_entitie[1]))\n",
        "  run7 = paragraph.add_run('\\nНайденные организации: ')\n",
        "  run7.font.color.rgb = RGBColor(0, 240, 0)\n",
        "  run7.italic = True\n",
        "  paragraph.add_run(', '.join(arr_entitie[2]))\n",
        "  paragraph = doc.add_paragraph()\n",
        "  run8 = paragraph.add_run('Данные найденные по ключевым словам из OpenAlex')\n",
        "  run8.font.size = Pt(16)\n",
        "  run8.bold = True\n",
        "  run8.underline = True\n",
        "  paragraph.space_after = Pt(12)\n",
        "  if \"Нет ключевых слов и фраз для поиска.\" == data_openalex_flag:\n",
        "    paragraph.add_run(\"\\nНет ключевых слов и фраз для поиска.\\n\")\n",
        "  else:\n",
        "    table_docx(data_openalex, doc)\n",
        "  doc.save('Отчет.docx')"
      ],
      "metadata": {
        "id": "sQyjmTZzNXNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Формирование отчета"
      ],
      "metadata": {
        "id": "10KiWVBcLrb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import plotly.express as px\n",
        "import plotly.io as pio\n",
        "\n",
        "def automatic_report(link, document_type):\n",
        "  \"\"\"\n",
        "  Формирует и анализирует предоставленный текст.\n",
        "\n",
        "  Параметры:\n",
        "  - link (str): ссылка на файл,\n",
        "  - document_type (int): тип документа.\n",
        "  \"\"\"\n",
        "  start_time1 = time.time()\n",
        "  text = extract_text(link)\n",
        "  end_time1 = time.time()\n",
        "  if text != \"Формат файла не соответствует!\" and document_type in [0, 1, 2, 3]:\n",
        "    #проверка документа на соответствие структуре ГОСТ\n",
        "    start_time2 = time.time()\n",
        "    result_gost = gost(text, document_type)\n",
        "    end_time2 = time.time()\n",
        "    # Учетные данные с сайта\n",
        "    userkey = 'ВАШ КЛЮЧ'\n",
        "    #очистка текста\n",
        "    cleaned_text = clean_text(text)\n",
        "    #отправка на проверку в антиплагиате\n",
        "    start_time3 = time.time()\n",
        "    if len(cleaned_text) > 150000:\n",
        "      answer = check_anti_plagiarism(userkey, cleaned_text[:1500]) #максимальное возможное количество для проверки в данном плагиате\n",
        "    else:\n",
        "      answer = check_anti_plagiarism(userkey, cleaned_text)\n",
        "    end_time3 = time.time()\n",
        "    if \"Произошла ошибка\" not in answer[0]:\n",
        "      numb = (answer[1][\"%\"].sum() / 100) / ((100 - float(answer[2])) / 100)\n",
        "      df_plagiat = answer[1]\n",
        "      df_plagiat[\"%\"] = answer[1][\"%\"].apply(lambda x: x / numb)\n",
        "      labels = df_plagiat['URL'].tolist() + ['Уникальный текст']\n",
        "      sizes = df_plagiat['%'].tolist() + [float(answer[2])]\n",
        "      # Создание кольцевой диаграммы с выделением одного сегмента\n",
        "      fig = px.pie(names=labels, values=sizes, hole=0.4)\n",
        "      fig.update_traces(pull=[0.1 if label == 'Уникальный текст' else 0 for label in labels], textposition='inside', insidetextorientation='radial')\n",
        "      fig.update_layout(legend=dict(orientation='h', yanchor='bottom', y=1.1, xanchor='left', x=0.1))\n",
        "      fig.show()\n",
        "    # Выделение ключевых слов и часто встречающихся из текста\n",
        "    start_time4_1 = time.time()\n",
        "    df_often = pd.DataFrame(sorted(Text_processing(cleaned_text).items(), key=lambda x: x[1], reverse=True), columns=['word', 'count'])\n",
        "    end_time4_1 = time.time()\n",
        "    # Орисовка облака слов\n",
        "    word_cloud(df_often[\"word\"])\n",
        "    # RAKE\n",
        "    stops_word = [\"менее\", \"более\", \"кроме\", \"должно\", \"должный\", \"данный\", \"детея\", \"которая\", \"который\"]\n",
        "    stops = list(set(stopwords.words(\"russian\") + stops_word))\n",
        "    start_time4_2 = time.time()\n",
        "    words1 = key_word_rake(text, stops, 5, 1)\n",
        "    end_time4_2 = time.time()\n",
        "    key_word1 = [i[0] for i in words1]\n",
        "    # YAKE\n",
        "    start_time4_3 = time.time()\n",
        "    words2 = key_word_yake(cleaned_text, 1, 5)\n",
        "    end_time4_3 = time.time()\n",
        "    key_word2 = [i[0].lower() for i in words2]\n",
        "    # TextRank\n",
        "    start_time4_4 = time.time()\n",
        "    words3 = key_word_textrank(cleaned_text, stops, 5)\n",
        "    end_time4_4 = time.time()\n",
        "    key_word3 = [i for i in words3]\n",
        "    # Topia\n",
        "    start_time4_5 = time.time()\n",
        "    key_word4, single_word_terms, multi_word_terms = topia(cleaned_text, 5)\n",
        "    end_time4_5 = time.time()\n",
        "    start_time4 = start_time4_1 + start_time4_2 + start_time4_3 + start_time4_4 + start_time4_5\n",
        "    end_time4 = end_time4_1 + end_time4_2 + end_time4_3 + end_time4_4 + end_time4_5\n",
        "    # Ключевые фразы\n",
        "    multi_df = pd.DataFrame(multi_word_terms[:15], columns=['word', 'count'])\n",
        "    # Лемматизация слов в каждом массиве\n",
        "    lemmatized_array1 = lemmatize_words(key_word1)\n",
        "    lemmatized_array2 = lemmatize_words(key_word2)\n",
        "    lemmatized_array3 = lemmatize_words(key_word3)\n",
        "    lemmatized_array4 = lemmatize_words(key_word4)\n",
        "    lemmatized_array5 = lemmatize_words(df_often['word'].iloc[:5].values)\n",
        "    # Ключевые слова + часто встречающиеся\n",
        "    res_word = lemmatized_array1 | lemmatized_array2 | lemmatized_array3 | lemmatized_array4 |lemmatized_array5\n",
        "    # Ключевые слова\n",
        "    key_word = lemmatized_array1 | lemmatized_array2 | lemmatized_array3 | lemmatized_array4\n",
        "    start_time5, start_time7 = 0, 0\n",
        "    end_time5, end_time7 = 0, 0\n",
        "    patent_flag, data_openalex_flag = True, True\n",
        "    if len(list(res_word)) != 0 or len(multi_df[\"word\"]) != 0:\n",
        "      # Поиск по Роспатенту\n",
        "      start_time5 = time.time()\n",
        "      patent = info_patent(list(res_word), multi_df[\"word\"], df_often['word'])\n",
        "      end_time5 = time.time()\n",
        "      len1 = 3 + len(list(res_word))\n",
        "      # Поиск по OpenAlex\n",
        "      start_time7 = time.time()\n",
        "      data_openalex = openalex(list(res_word) + list(multi_df[\"word\"].values))\n",
        "      end_time7 = time.time()\n",
        "      len2 = len(list(res_word) + list(multi_df[\"word\"].values))\n",
        "    else:\n",
        "      patent_flag = False\n",
        "      data_openalex_flag = False\n",
        "    # Поиск именнованных сущностей\n",
        "    start_time6 = time.time()\n",
        "    arr_entitie = entities(cleaned_text)\n",
        "    end_time6 = time.time()\n",
        "    # Формирование отчета\n",
        "    start_time8 = time.time()\n",
        "    report_docx(result_gost, answer, df_often, key_word, multi_df, patent, arr_entitie, data_openalex, patent_flag, data_openalex_flag)\n",
        "    end_time8 = time.time()\n",
        "    dict_type_doc = {0: \"Техническое задание\",\n",
        "                     1: \"Пояснительная записка\",\n",
        "                     2: \"Отчет НИР\",\n",
        "                     3: \"Отчет об опытной эксплуатации\"}\n",
        "    return link.split('/')[-1], dict_type_doc[document_type], len(text), len(cleaned_text), end_time1 - start_time1, end_time2 - start_time2, end_time3 - start_time3, end_time4 - start_time4, end_time5 - start_time5, end_time6 - start_time6, end_time7 - start_time7, end_time8 - start_time8, end_time4_1 - start_time4_1, end_time4_2 - start_time4_2, end_time4_3 - start_time4_3, end_time4_4 - start_time4_4, end_time4_5 - start_time4_5, len1, len2\n",
        "  else:\n",
        "    print(\"Невозможно создать отчет, проверьте подаваемый документ!\")\n",
        "    return \"ОШИБКА\""
      ],
      "metadata": {
        "id": "fgBo4GDQ_UH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Анализ"
      ],
      "metadata": {
        "id": "8xHXXM7zFPxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "arr_link = [\"Документ 1\",\n",
        "            \"Документ 2\",\n",
        "            \"Документ 3\",\n",
        "            \"Документ 4\",\n",
        "            \"Документ 5\",\n",
        "            \"Документ 6\",\n",
        "            \"Документ 7\",\n",
        "            \"Документ 8\",\n",
        "            \"Документ 9\",\n",
        "            \"Документ 10\",\n",
        "            \"Документ 11\",\n",
        "            \"Документ 12\"]\n",
        "arr_type = [0,0,0,0,0,1,1,2,2,3,3,3]"
      ],
      "metadata": {
        "id": "joU6h_3Kg5YF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict_type_doc = {0: \"Техническое задание\",\n",
        "                     1: \"Пояснительная записка\",\n",
        "                     2: \"Отчет НИР\",\n",
        "                     3: \"Отчет об опытной эксплуатации\"}"
      ],
      "metadata": {
        "id": "lzaLZMt6cVWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1 = pd.DataFrame({'Документ': [], 'Тип документа': [], 'Количество символов': [], 'Количество символов после очистки': [], 'Время перевода документа в машинночитаемый вид': [], 'Время проверки на соответствие структуре ГОСТ': [], 'Время проверки документа в системе антиплагиат': [], 'Общее время выделения ключевых и часто встречающихся слов и фраз': [], 'Время поиска по Роспатенту': [], 'Время поиска именнованных сущностей': [], 'Время поиска по OpenAlex': [], 'Время формирования отчета': [], \"Количество получившихся запросов к Роспатенту\": [], \"Количество получившихся запросов к OpenAlex\": []})\n",
        "result2 = pd.DataFrame({'Документ': [], 'Тип документа': [], 'Общее время выделения часто встречающихся слов и фраз': [], 'Время выделения часто встречающихся слов': [], 'Время выделения ключевых слов методом RAKE': [], 'Время выделения ключевых слов методом YAKE': [], 'Время выделения ключевых слов методом TextRank': [], 'Времы выделения ключевых слов методом Topia': []})"
      ],
      "metadata": {
        "id": "fNEBW09y1kzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, j in zip(arr_link, arr_type):\n",
        "  res = automatic_report(i, j)\n",
        "  if res != \"ОШИБКА\":\n",
        "    result1.loc[len(result1.index)] = [res[0], res[1], res[2], res[3], res[4],  res[5], res[6], res[7], res[8], res[9], res[10], res[11], res[17], res[18]]\n",
        "    result2.loc[len(result2.index)] = [res[0], res[1], res[7], res[12],  res[13],  res[14],  res[15],  res[16]]\n",
        "    result1.to_excel(\"адрес сохранения\")\n",
        "    result2.to_excel(\"адрес сохранения\")\n",
        "  else:\n",
        "    print(f\"Ошибка при обработки документа {link}\")"
      ],
      "metadata": {
        "id": "YsLFIP9CE_FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "result1 = pd.read_excel(\"адрес сохранения\")\n",
        "result2 = pd.read_excel(\"адрес сохранения\")\n",
        "result1 = result1.drop('Unnamed: 0', axis=1)\n",
        "result2 = result2.drop('Unnamed: 0', axis=1)"
      ],
      "metadata": {
        "id": "ycl_lb7yTxbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result1.iloc[:, 2:].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YrLNw765Dv5",
        "outputId": "c359cc34-8a83-4fea-dea7-b6d43986aee4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Количество символов                                                 178317.916667\n",
              "Количество символов после очистки                                   163990.083333\n",
              "Время перевода документа в машинночитаемый вид                           1.285627\n",
              "Время проверки на соответствие структуре ГОСТ                            0.073782\n",
              "Время проверки документа в системе антиплагиат                          84.719000\n",
              "Общее время выделения ключевых и часто встречающихся слов и фраз        46.842275\n",
              "Время поиска по Роспатенту                                              98.995493\n",
              "Время поиска именнованных сущностей                                     43.933758\n",
              "Время поиска по OpenAlex                                                12.603817\n",
              "Время формирования отчета                                                0.415532\n",
              "Количество получившихся запросов к Роспатенту                           18.750000\n",
              "Количество получившихся запросов к OpenAlex                             29.916667\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2.iloc[:, 2:].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_EYU5ir86Wl",
        "outputId": "b097f9c3-8dd3-4aec-e409-b643589f59d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Общее время выделения часто встречающихся слов и фраз    46.842275\n",
              "Время выделения часто встречающихся слов                  4.522322\n",
              "Время выделения ключевых слов методом RAKE                0.301547\n",
              "Время выделения ключевых слов методом YAKE                1.873420\n",
              "Время выделения ключевых слов методом TextRank           31.867459\n",
              "Времы выделения ключевых слов методом Topia               8.277528\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_sum = result1.columns[4:]\n",
        "row_sums = result1[columns_to_sum].sum(axis=1)\n",
        "print(\"Суммы для каждой строки:\")\n",
        "print(row_sums)\n",
        "print(f\"Среднее значение всех сумм: {row_sums.mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKT-BJar85an",
        "outputId": "d8cae13d-6b20-4b01-a11c-9da0496ef585"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Суммы для каждой строки:\n",
            "0     233.694684\n",
            "1     329.859509\n",
            "2     304.619835\n",
            "3     341.871475\n",
            "4     295.120563\n",
            "5     216.840783\n",
            "6     320.051265\n",
            "7     416.430831\n",
            "8     363.665913\n",
            "9     505.750322\n",
            "10    376.946660\n",
            "11    345.579582\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 337.53595185415975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for group_name, group_data in result1.groupby(result1[result1.columns[1]]):\n",
        "    row_sums = group_data[result1.columns[4:]].sum(axis=1)\n",
        "    print(f\"Группа: {group_name}\")\n",
        "    print(\"Суммы для каждой строки:\")\n",
        "    print(row_sums)\n",
        "    print(f\"Среднее значение всех сумм: {row_sums.mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXPBI_2FAcL0",
        "outputId": "b10534f6-c134-49b9-fa0e-1daf71482ad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Группа: Отчет НИР\n",
            "Суммы для каждой строки:\n",
            "9     505.750322\n",
            "10    376.946660\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 441.3484911311492\n",
            "Группа: Отчет об опытной эксплуатации\n",
            "Суммы для каждой строки:\n",
            "4     295.120563\n",
            "5     216.840783\n",
            "11    345.579582\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 285.84697600682347\n",
            "Группа: Пояснительная записка\n",
            "Суммы для каждой строки:\n",
            "0    233.694684\n",
            "2    304.619835\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 269.15725936889646\n",
            "Группа: Техническое задание\n",
            "Суммы для каждой строки:\n",
            "1    329.859509\n",
            "3    341.871475\n",
            "6    320.051265\n",
            "7    416.430831\n",
            "8    363.665913\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 354.375798645871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "337.53595185415975 / 60"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmLadaEJ9gEl",
        "outputId": "689d04f2-2d77-4e36-95bc-173de5fceea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5.625599197569329"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result2.iloc[:, 2:].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B82CNrSFFzrd",
        "outputId": "5fa56f8a-ca06-4ae8-ee25-5886d33bbc13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Общее время выделения часто встречающихся слов и фраз    46.842275\n",
              "Время выделения часто встречающихся слов                  4.522322\n",
              "Время выделения ключевых слов методом RAKE                0.301547\n",
              "Время выделения ключевых слов методом YAKE                1.873420\n",
              "Время выделения ключевых слов методом TextRank           31.867459\n",
              "Времы выделения ключевых слов методом Topia               8.277528\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for group_name, group_data in result2.groupby(result2[result2.columns[1]]):\n",
        "    row_sums = group_data[result2.columns[3:]].sum(axis=1)\n",
        "    print(f\"Группа: {group_name}\")\n",
        "    print(\"Суммы для каждой строки:\")\n",
        "    print(row_sums)\n",
        "    print(f\"Среднее значение всех сумм: {row_sums.mean()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9krJaQ5TG84S",
        "outputId": "39cc262a-4746-40d4-bc71-fcc82adbdfbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Группа: Отчет НИР\n",
            "Суммы для каждой строки:\n",
            "9     185.592427\n",
            "10     48.993207\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 117.29281675815524\n",
            "Группа: Отчет об опытной эксплуатации\n",
            "Суммы для каждой строки:\n",
            "4     21.317083\n",
            "5      6.522424\n",
            "11    90.491787\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 39.44376452763872\n",
            "Группа: Пояснительная записка\n",
            "Суммы для каждой строки:\n",
            "0     7.879074\n",
            "2    31.415858\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 19.64746594429016\n",
            "Группа: Техническое задание\n",
            "Суммы для каждой строки:\n",
            "1    23.326828\n",
            "3    25.873461\n",
            "6    28.336969\n",
            "7    65.190987\n",
            "8    27.167203\n",
            "dtype: float64\n",
            "Среднее значение всех сумм: 33.979089450836184\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_round = result1.columns[2:]\n",
        "result1[columns_to_round] = result1[columns_to_round].round(decimals=1)\n",
        "file_path = \"адрес сохранения\"\n",
        "result1.to_excel(file_path, index=False)"
      ],
      "metadata": {
        "id": "Bj3UMPn9u17J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_round = result2.columns[2:]\n",
        "result2[columns_to_round] = result2[columns_to_round].round(decimals=1)\n",
        "file_path = \"адрес сохранения\"\n",
        "result2.to_excel(file_path, index=False)"
      ],
      "metadata": {
        "id": "l0f26UGEvRCn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}